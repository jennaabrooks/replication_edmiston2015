---
title: "Replication of Study What makes words special? Words as unmotivated cues by Pierce Edmiston & Gary Lupyan (2015, Cognition)"
author: "Noah Khaloo"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
editor: 
  markdown: 
    wrap: 72
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

**GitHub Repository found here:**
<https://github.com/ucsd-psych201a/edmiston2015>

**Link to experimental paradigm**:
<https://ucsd-psych201a.github.io/edmiston2015/>

**Link to preregistration draft:**
<https://drive.google.com/drive/folders/1DR8HreGxv_Ct92-FIszRW-qtfVbgpE4K>

Edminston & Lupyan (2015) provides evidence towards notion that
ambiguous labels (i.e., "dog" or "guitar") activate knowledge of their
referent quicker than environmental sounds. For example, a listener
should be able to point out a dog quicker if they hear the label "dog",
rather than if they hear the sound of a dog's bark. The reason for this,
Edminston & Lupyan (2015) state, is due to the fact that environmental
sounds activates a "particular instance of a category". In other words,
a dog's bark may have qualities that a listener may contribute to a
particular type of dog (i.e., a low pitched bark may be more indicative
of a big dog, etc.). Edminston & Lupyan (2015) demonstrate this effect
by showing that participants take longer to recognize a picture of a
referent after hearing a sound associated with said referent than they
do upon hearing the label.

In our replication, we will be revisiting experiment "1A" from the
Edminston & Lupyan (2015) paper. We will set up an online
picture-recognition task using the same materials found in the original
paper. We will try and replicate the experiment exactly, manipulating
the same mappings between sounds, labels, and pictures.

## Methods

Participants: We will be shooting for around 2.5x the participants that
Edminston & Lupyan (2015) had (see Power Analysis). They only targetted
undergraduates, but, since we have an online study, we can target anyone
who is willing to take the study, thus testing the generalizability of
Edminston & Lupyan's (2015) results.

Materials: We will use the same materials as Edminston & Lupyan (2015):
labels and environmental sounds for six categories: bird, dog, drum,
guitar, motorcycle, and phone. Each category will have two separate
images and two separate environmental sounds. We will also implement a
male and female voice for our spoken labels.

Procedure: We will follow the same procedure as Edminston & Lupyan
(2015). Participants will hear a cue and see a picture, and will be
prompted to decide as "quickly and accurately as possible if the picture
they saw came from the same basic-level category as the word or sound
they heard" (Edminston & Lupyan pp.94). Following Edminston & Lupyan
(2015), some environmental sounds will be congruent with the picture
(i.e., a big dog matched with a low-pitched bark). These will be labeled
as "congruent sounds". Other environmental sounds will be deemed
"incongruent sounds", in which the picture does not necessarily match
the environmental sounds (i.e., a low pitched bark matched with a
picture of a small dog).

### Power Analysis

Since their paper used a mixed effects linear regression model, we were
not able to straightforwardly conduct a power analysis. In order to
mitigate this, we just intend on collecting 2.5x the amount of
participants as they did. Since they collected 43 participants, we will
collect \~108 participants. This should be a large enough sample size to
examine whether the effect they report holds up.

### Planned Sample

In their original study, Edminston & Lupyan (2015) used "43 University
of Wisconsin–Madison undergraduates participated in Experiment 1A in
exchange for course credit.”

For the current study, we will aim for \~108 participants, 2.5x the
amount used in the original paper.

### Materials

From the original study: "The auditory cues comprised basic-level
category labels and environmental sounds for six categories: bird, dog,
drum, guitar, motorcycle, and phone. For each category, we obtained two
distinct environmental sound cues, e.g., <classical guitar strum>,
<electric guitar strum>, and two separate images for each subordinate
category, e.g., two electric guitars for <electric guitar strum>, two
acoustic guitars for <electric guitar strum>. To control for cue
variability, we also used two versions of each spoken category label:one
pronounced by a female speaker, one by a male speaker. All auditory cues
were equated in duration (600 ms.) and normalized in volume. The images
were color photographs (four images per category)." (pp. 94)

### Procedure

From the original study: "On each trial participants heard a cue and saw
a picture. We instructed participants to decide as quickly and
accurately as possible if the picture they saw came from the same
basic-level category as the word or sound they heard Participants were
tested in individual rooms sitting approximately 2400 from a monitor
such that images subtended 10 10. Trials began with a 250 ms. fixation
cross followed immediately by the auditory cue, delivered via
headphones. The target image appeared centrally 1 s after the offset of
the auditory cue and remained visible until a response was made. Each
participant completed 6 practice and 384 test trials. If the picture
matched the auditory cue (50% of trials) participants were instructed to
respond ‘Yes’ on a gaming controller (e.g., <cellphone ring> or
‘‘phone’’ followed by a picture of any phone). Otherwise, they were to
press ‘No’ (e.g., <cellphone ring> or ‘‘phone’’ followed by a dog). All
factors (cue type, congruence) varied randomly within subjects. Auditory
feedback (buzz or bleep) was given after each trial." (pp. 94)

### Analysis Plan

From the original study: “All participants performed very accurately on
all items (M = 97%). Response times (RTs) shorter than 250 ms. or longer
than 1500 ms. were removed (292 trials removed, 1.77% of total). We fit
RTs for correct responses on matching trials (‘Yes’ responses) with
linear mixed regression using maximum likelihood estimation (Bates,
Maechler, Bolker, & Walker, 2013), including random intercepts and
random slopes for within-subject factors and random intercepts for
repeated items (unique trial types) following the recommendations of
Barr, Levy, Scheepers, and Tily (2013). Reported below are the parameter
estimates (b) and confidence intervals for each contrast of interest.
Significance tests were calculated using chi-square tests that compared
nested models—models with and without the factor of interest—on
improvement in log-likelihood.” (pp.94/95)

In our replication, we will follow their analysis as closely as
possible: we will remove trials where response time is shorter than 250
ms or longer than 1500ms. We will also try to closely replicate the same
linear mixed-effects regression model, including the same main effects
and random structure. Furthermore, we will also implement the
chi-squared tests they ran that compared models with-and without the
main effect using log-likelihood. We predict that their results will
replicate; that verbal labels will elicit faster responses than sounds,
and that congruent sounds will eleicit faster response times that
incongruent sounds.

**Clarify key analysis of interest here** Linear mixed regression model,
chi-square tests of nested models with and without the factor of
interest.

### Differences from Original Study

Our project will be online, while there's was in-person, in a lab
setting. One thing we have to consider that might influence reaction
time in our replication is internet speed/technical issues that may
arise while participants take our online study. Furthermore,
participants may be more prone to distractions, as we are unable to
control their location. In addition, our participants will have to use
their keyboards on their computers to react to stimuli, while, in the
original study, participants used a video-game controller. This may
result in a difference in reaction time in either direction that we are
unable to straightforwardly control for exactly.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data
collection.

#### Actual Sample

Sample size, demographics, data exclusions based on rules spelled out in
analysis plan

#### Differences from pre-data collection methods plan

none

## Design Review

1 factor was directly manipulated in Edminston & Lupyan (2015). This was
the auditory cue participants heard, which could either be a label, a
congruent environmental sound, or an incongruent environmental sound.

One measurement was taken, which was reaction time of the participants.

They used a within-participant design, subjecting participants to the
same condition

The measures were repeated. 384 trials

There would be more confounds, and a difference in reaction time if the
experimental design was switched.

They did not mention any direct measures to lessen the impact of demand
characteristics.

They should have added some attention checks, given how many trials they
had. I also would have kept the "incorrect" trials, given the fact that
the incongruent labels may have resulted in a "no" response, which could
be experimentally valid.

A confound could have been that they did not use a varied enough sample,
more specifically, their sample was primarily undergraduates, which
could mean that their population was skewed more towards the WEIRD
population.

## Pilot A

For Pilot A, we closely followed the experiment described in the
original study. The GitHub page for our paradigm is linked below:

Link to experimental paradigm:
https://ucsd-psych201a.github.io/edmiston2015/

For our pilot, we collected data from three participants that we
recruited using personal connections. This data was imported and
uploaded to the data folder in the project repository, which can be
found on our GitHub repoisitory here:

https://github.com/ucsd-psych201a/edmiston2015/tree/main/data

## Pilot B

For pilot B, we collected 5 participants total, recruited via prolific.
The data was uploaded to a folder in our github repo, which can be found
here:

<https://github.com/ucsd-psych201a/edmiston2015/tree/main/data/pilot_b>

median time it took participants to do the experiment: 25 minutes (via
Prolific)

## Results

### Data preparation

As mentioned previously, we will be replicating experiment 1A from
Edminston & Lupyan (2015). As for our data preparation, we will be
making sure to label the type of relationship between the sound
participants hear in a given trial, and its match to the picture. In
other words, we will define whether a particular trial is a label (which
will just be a word such as "dog, or "guitar"), congruent sound, or
incongruent sound. Furthermore, we will use the exact same audio and
visual stimuli as Edminston & Lupyan (2015). All of the different audio
files have already been clipped to make them match in duration, and they
have all been amplitude normalized.

After we have collected our data, we will filter outliers in the exact
same way as Edminston & Lupyan (2015). More specifically, we will filter
out any reaction time (rt) that is lower that 250 ms, and any that is
greater than 1500 ms. It also seems as though Edminston & Lupyan (2015)
filtered out incorrect responses, but I am not certain about this.

```{r}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(readr)
library(lme4)

#### Import data
#replace this with your own path (for now)
folder_path <- "../data/complete"
csv_files <- list.files(folder_path, pattern = "*.csv", full.names = TRUE)
df_list <- lapply(csv_files, read_csv, show_col_types = FALSE)
original_df <- bind_rows(df_list)
combined_df <- original_df

#exclude practice trials 
combined_df <- combined_df %>%
  filter(exp_part == "actual")

#### Data exclusion / filtering
#exclude 'No' trials 
combined_df <- combined_df %>%
  mutate(correct_response = as.character(correct_response)) %>%
  mutate(response = as.character(response)) %>%
  mutate(correct = correct_response == response)

accuracy_table <- combined_df %>%
  group_by(ID) %>%
  summarise(accuracy = mean(correct, na.rm = TRUE))
accuracy_table

# QA
accuracy_thresh <- 0.9
# select people who passed QA
id_pass_qa <- accuracy_table %>%
  filter(accuracy > accuracy_thresh) %>%
  pull(ID)
  
# filter combined_df
combined_df <- combined_df %>%
  filter(ID %in% id_pass_qa) %>%
  select(-correct) # drop the correctness column

#check whether response is correct
combined_df <- combined_df %>%
  filter(response == "f") %>%
  filter(correct_response == "f")

#rename "sound_subtype" to "cue"
combined_df <- combined_df %>%
  rename(cue = sound_subtype)

#create congruency column 
combined_df <- combined_df %>%
  mutate(congruency = case_when(
    cue == "label" ~ "label",
    img_subtype %in% c("song", "york", "bongo", "acoustic", "harley", "rotary") & sound_version == "A" ~ "incongruent",
    TRUE ~ "congruent"
  ))


#filter reaction time 
combined_df <- combined_df %>%
  filter(rt >250, rt <1500)

#### Prepare data for analysis - create columns etc.
combined_df <- combined_df %>%
  select(rt, ID, sound_category,cue, congruency)

#show the preprocesssed data
tail(combined_df)

```
Distribution of accuracy
```{r}
library(ggplot2)

# Create the plot
ggplot(accuracy_table, aes(x = accuracy)) +
  geom_histogram(
    breaks=seq(0, 1, length.out=21),
    fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = accuracy_thresh, color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Distribution of Accuracy",
    x = "Accuracy",
    y = ""
  ) +
  theme_minimal()

```
Distribution of response time
```{r}
library(ggplot2)

original_df_with_good_subj <- original_df %>%
  filter(ID %in% id_pass_qa)

# Create the plot
ggplot(original_df_with_good_subj, aes(x = rt)) +
  geom_histogram(
    breaks=seq(0, 2000, length.out=40),
    fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = 250, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = 1500, color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Distribution of Reaction Time",
    x = "Reaction Time",
    y = ""
  ) +
  theme_minimal()

```

More stats on people performance (i.e., how many people pass the quality check).
```{r}
pass_thresh_rate <- accuracy_table %>%
  mutate(pass=accuracy > accuracy_thresh) %>%
  summarise(mean=mean(pass))

pass_thresh_rate
```

For each congruency type...
```{r}
original_df_with_congruency <- original_df_with_good_subj |>
  filter(img_category == sound_category) |>
  mutate(congruency = case_when(
    sound_subtype == "label" ~ "label",
    img_subtype %in% c("song", "york", "bongo", "acoustic", "harley", "rotary") & sound_version == "A" ~ "incongruent",
    TRUE ~ "congruent"
  )) |>
  mutate(correct=response=='f') |>
  group_by(ID, congruency) |>
  summarize(mean_acc=mean(correct)) |>
  ungroup()

original_df_with_congruency |>
  group_by(congruency) |>
  summarise(
    congruency_mean_acc=mean(mean_acc), 
    congruency_std_acc=sd(mean_acc))
```

More stats on response time
```{r}
ecdf_func <- ecdf(original_df_with_good_subj$rt)
quantiles <- ecdf_func(c(250, 1500))
quantiles
```

compute effect size (congruent- incongruent) 
```{r}
# see if there are differences within each sub-type
subtype_avg <- combined_df |>
  filter(congruency != 'label') |>
  group_by(sound_category, congruency, ID) |>
  summarise(mean_rt=mean(rt)) |>
  ungroup()

diff_table <- subtype_avg |>
  group_by(ID, sound_category) |>
  filter(all(c("congruent", "incongruent") %in% congruency)) |>
  summarize(
    mean_rt_diff = mean_rt[congruency == "congruent"] - mean_rt[congruency == "incongruent"],
    .groups = "drop"
  ) |>
  ungroup()

# keep only those with 6 groups
diff_table <- diff_table %>%
  group_by(ID) %>%
  filter(n() == 6) %>% # Keep only IDs with more than one observation
  ungroup()

# set congruent to back reference level
library(broom)

# Perform one-sample t-tests
effect_per_type <- diff_table %>%
  group_by(sound_category) %>%
  summarise(
    t_test = list(t.test(mean_rt_diff, mu = 0))
  ) %>%
  mutate(
    tidied = map(t_test, tidy)
  ) %>%
  unnest(tidied)

effect_per_type
```

### Confirmatory analysis

Following Edminston & Lupyan (2015), we will be fitting mixed effects
linear regression models to our response time data (which will be
filtered for correct responses on matching trials (i.e., 'Yes'
responses)). The model will include random intercepts and random slopes
for within-subject factors (i.e. "subject"), and random intercepts for
repeated items (i.e. what item was used (bird, guitar, dog, etc.)). The
main effect (or "contrast of interest") of the model will be the
"condition" variable, which will denote whether a particular trial
presented a label, congruent sound, or incongruent sound. We will report
the parameter estimates and confidence intervals for each "contrast of
interest" (pp.94). Furthermore, we will calculate chi-square tests that
will "compare nested models--models with and without the factor of
interest--on improvement in log-likelihood" (pp.94).

Our analysis presented here tests the main inference of the paper, which
is that condition effects reaction time. More specifically, by comparing
our parameter estimates for condition = label, condition = congruent,
and condition = incongruent, we can get a sense of exactly how the
different conditions influence reaction time. Crucially, the model
allows us to compare the effect on reaction time each condition has to
each other We expect that the output of our model will look very similar
to the results that Edminston & Lupyan (2015) present.

Furthermore, creating a new model without this factor of interest and
comparing it performance to the full model will allow us to better
understand if the factor of interest truly accounts for the trends we
see.

*Side-by-side graph with original graph is ideal here*

```{r}
library(lmerTest)
library(emmeans)
library(lme4)

#recreation of fig 2
library(ggplot2)

# Reorder the 'congruency' factor levels
combined_df$congruency <- factor(combined_df$congruency, levels = c("label", "congruent", "incongruent"))

ggplot(data = combined_df,
       mapping = aes(x = congruency,
                     y = rt,
                     color = congruency, 
                     fill = congruency)) +
  geom_bar(stat = "summary", fun = "mean", 
           position = position_dodge(width = 0.6), width = 0.4, 
           color = "black") +  # Dark bands around the bars (black border)
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "linerange", 
               position = position_dodge(width = 0.6), color = "black") +
  scale_fill_manual(values = c("darkred", "darkblue", "lightblue")) +
  scale_color_manual(values = c("darkred", "darkblue", "lightblue")) +
  scale_x_discrete(labels = c("label" = "Label", 
                              "congruent" = "Congruent", 
                              "incongruent" = "Incongruent")) +  # Renaming x-axis labels
  labs(
    x = "Cue Type",
    y = "Reaction Time (Ms)",
    color = "Cue Type",
    fill = "Cue Type"
  ) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(panel.background = element_blank(), 
        panel.grid = element_blank(), 
        axis.line = element_line(color = "black"))



#statistical analysis 

#find best optimizer
#allFit(model_full)

# set congruent to back reference level
combined_df$congruency <- relevel(combined_df$congruency, ref = "congruent")

#set up model and view results
model_full <- lmer(rt ~ congruency + (1 + congruency|ID) + (1|sound_category), data = combined_df, control = lmerControl(optimizer = "bobyqa"))

summary(model_full)
```

```{r}
#get 95% CI
confint.merMod(model_full,method="Wald")


# #post-hoc test: allows you to examine relationship between incongruent and label
#####. 95% CI for this??? ###3
model_full %>%
  emmeans(pairwise ~ congruency,
          #adjusts p values so that it is more difficult to get a significance (correction method)
          adjust = "bonferroni", 
          conf.int = TRUE) %>%
  pluck("contrasts")



# #chi squared test 
model_reduced <- lmer(rt ~ (1 + congruency|ID) + (1|sound_category), data = combined_df)
anova(model_full, model_reduced)

```

### Exploratory analyses

Any follow-up analyses desired (not required).

## Discussion

## Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary
result from the confirmatory analysis and the assessment of whether it
replicated, partially replicated, or failed to replicate the original
result.

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from
follow-up exploratory analysis, (b) assessment of the meaning of the
replication (or not) - e.g., for a failure to replicate, are the
differences between original and present study ones that definitely,
plausibly, or are unlikely to have been moderators of the result, and
(c) discussion of any objections or challenges raised by the current and
original authors about the replication attempt. None of these need to be
long.
