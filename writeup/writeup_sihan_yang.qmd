---
title: "Replication of Study 'What makes words special? Words as unmotivated cues' by Edmiston & Lupyan (2015, Cognition)"
author: "Replication Author[s]: Sihan Yang (siy009@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

How language supports cognition is a centeral question in cognitive linguistics. Previous research suggests that language may play an unique role in the mental processing of categories. Lupyan and Thompson-Schill (2012) found that, during an image-recognition task, participants judged whether an image and a preceeding auditory cue were of the same category faster when the cue was verbal rather than non-verbal. For instance, people identified a guitar image as matching the word 'guitar' faster than the sound of an acoustic guitar playing. 

Edmiston and Lupyan (2015) further explored why verbal cues activate concepts more effectively. They proposed that verbal cues are associated with the abstract concept rather than specific instances, thereby cueing more diagnostic features of a category. In their experiment 1A, they found that in an image-recogniton task, participants (1) responded faster to an image if the cue is a verbal label than an environmental sound, and (2) responded faster to an image if the environmental sound cue is a congruent one (i.e., matching the instance in the image) than an incongruent one (i.e., matching the category but not the exact instance). These findings suggest the potential mechanism by which language supports conceptual thinking.

Our study aims to replicate Experiment 1A from Edmiston and Lupyan (2015). We define the replication as successful if two conditions are met: (1) the two relationships between reaction time under different auditory cue conditions are the same as described above, and (2) the differences in reaction time are statistically attributable to the type of auditory cue. Specifically, we expect reaction times to follow the pattern: verbal-label condition < congruent-environmental-sound condition < incongruent-environmental-sound condition, with statistical tests confirming that auditory cue type is the primary determinant of these differences.

Our study will employ the same experimental paradigm and materials as the original study but will be conducted online rather then in a laboratory setting. While this adjustment may lead to different sample population and slight procedural variations, we do not anticipate the changes substantially affect the results. We hope the replication to provide insight into the reliability of original findings and their broader implications for understanding the relationship between language and cognition.

Followings are the the links to our repository, pre-registration, and hosted experiment:
[Project Github repository](https://github.com/ucsd-psych201a/edmiston2015)
[Study pre-registration](https://osf.io/7ca8z)
[Experiment link](https://ucsd-psych201a.github.io/edmiston2015/)

## Methods

### Power Analysis
Since the original study did not provide means and standard deviations for the effects and employed a linear mixed-effects regression model, it is challenging to conduct a conventional power analysis to estimate a proper sample size.
However, based on guidance from course instructional staff, the original sample size ($N=43$) should be adequate to acheiving 80% power for detecting the effect reported in the original study, with a significance criteria of $\alpha=0.05$. 

### Planned Sample

Given that the original study is well-powered ($N=43$) but in a laboratory setting, we choose a sample size slightly larger ($N=50$) sample size to account for the uncertainty in online settings. Participants are recruited through Prolific, prescreened for English fluency, and compensated monetarily for their participation.

### Materials

We use the exact same images and auditory cues as in the original study. The following are the detailed explanation from the authors: 
"The auditory cues comprised basic-level category labels and environmental sounds for six categories: bird, dog, drum, guitar, motorcycle, and phone. For each category, we obtained two distinct environmental sound cues, e.g., <classical guitar strum>, <electric guitar strum>, and two separate images for each subordinate category, e.g., two electric guitars for <electric guitar strum>, two acoustic guitars for <electric guitar strum>. To control for cue variability, we also used two versions of each spoken category label: one pronounced by a female speaker, one by a male speaker. All auditory cues were equated in duration (600 ms.) and normalized in volume. The images were color photographs (four images per category). The materials, obtained from online repositories, are available for download at http://sapir.psych.wisc.edu/stimuli/ MotivatedCuesExp1A-1B.zip." (Edmiston & Lupyan, 2015)

### Procedure	

Participants are instructed to use their personal computer to complete task in a quite environment absent of distraction. During the experiment, they will be required to make their response by clicking 'f' on the keyboard of their computer for 'Yes', and 'j' for 'No'.

During the experiment, "on each trial participants heard a cue and saw a picture...participants decide as quickly and accurately as possible if the picture they saw came from the same basic-level category as the word or sound they heard...Trials began with a 250 ms. fixation cross followed immediately by the auditory cue, delivered via headphones. The target image appeared centrally 1 s after the offset of the auditory cue and remained visible until a response was made. Each participant completed 6 practice and 384 test trials. If the picture matched the auditory cue (50% of trials) participants were instructed to respond ‘Yes’...(e.g., <cellphone ring> or 'phone' followed by a picture of any phone). Otherwise, they were to press 'No' (e.g., <cellphone ring> or 'phone' followed by a dog). All factors (cue type, congruence) varied randomly within subjects. Auditory feedback (buzz or bleep) was given after each trial." (Edmiston & Lupyan, 2015)

### Analysis Plan

#### Data cleaning and Data Exclusion Rules

Before testing the main hypothesis of the original study, we will first examine the average performance (accuracy) to determine if it is comparable to the original study. This will involve calculating each participant's accuracy and using a t-test to examine whether performance differs qualitatively between the original and new participants. Given that the average performance is 97% in the original study, we set the data exclusion threshold at 90%: only participants who have achieved an accuracy rate above 90% will be included in further analysis.

Trials with response times (RTs) that are too short or too long will be filtered out. The original study applied thresholds of 250 ms and 1500 ms for the lower and upper bounds, respectively. Before applying filtering, we will check the distribution of RT.

Finally, only RTs for correct responses on matching trials will be used for confirmatory analysis, following the original study’s approach.

#### Covariates

The main covariate in this experiment will be "the presence of incongruent sound trials (e.g., hearing a <cell phone ring> and responding ‘Yes’ if a rotary phone was presented) may have led participants to be more careful on these trials, inflating the RTs" (Edmiston & Lupyan, 2015). However, as this factor is not the focus of the experiment to replicate and was addressed in another following experiment in the original study, we will not include it in our analysis.

#### Key Analysis

We will "fit RTs for correct responses on matching trials (‘Yes’ responses)" under three conditions of different cues (verbal label cue, congruent sound cue, incongruent sound cue), "with linear mixed regression using maximum likelihood estimation (Bates, Maechler, Bolker, & Walker, 2013), including random intercepts and random slopes for within-subject factors and random intercepts for repeated items (unique trial types) following the recommendations of Barr, Levy, Scheepers, and Tily (2013)." (Edmiston & Lupyan, 2015). 

We will report the "parameter estimates (b) and confidence intervals for each contrast of interest" (Edmiston & Lupyan, 2015). Significance tests will be calculated "using chi-square tests that compared nested models—models with and without the factor of interest—on improvement in log-likelihood" (Edmiston & Lupyan, 2015).

According to the original study, we predict that the analysis above should show that (1) verbal labels elicit faster responses than sound labels, and (2) congruent sounds elicit faster response than incongruent sounds, with statistical significance (indicated by chi-square test p value less than 0.05).

### Differences from Original Study

#### Sample

While the original study recruited participants on campus, our recruitment is conducted on a public global outsourcing platform, expanding the sampling pool from U.S. college undergraduates to a more diverse population.

#### Procedure

While the original study was conducted in person in a lab, our replication will be hosted online, allowing participants to complete the task independently at a location of their choice, without the supervision of experimenters. In addition, responses will be made using a keyboard rather than a game controller.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.

## Results

### Data preparation

Data preparation following the analysis plan.

#### Pilot A Test (for Project Checkpoint 1)

We conducted pilot tests involving three participants. All data are collected using the [online experiment link](https://ucsd-psych201a.github.io/edmiston2015/). The data is available [here](https://github.com/ucsd-psych201a/edmiston2015/tree/main/data/pilot_a). 

#### Pilot B Test (for Project Checkpoint 3)

We conducted pilot tests involving five participants. All data are collected using Prolific. The data is available [here](https://github.com/ucsd-psych201a/edmiston2015/tree/main/data/pilot_b). The average time participant took to complete the study was 25 minutes.

#### Data cleaning and Data Exclusion Rules

Before testing the main hypothesis of the original study, we will first examine the average performance (accuracy) to determine if it is comparable to the original study. This will involve calculating each participant's accuracy and using a t-test to examine whether performance differs qualitatively between the original and new participants.

Trials with response times (RTs) that are too short or too long will be filtered out. The original study applied thresholds of 250 ms and 1500 ms for the lower and upper bounds, respectively. Before applying filtering, we will check if the new data shows a similar RT distribution.

Only RTs for correct responses on matching trials will be used for further analysis, following the original study’s approach.

The following code prepare the data for fitting a linear mixed model for response time under different conditions: (1) label cue matches the image, (2) auditory cue matches the image but is incongruent, and (3) auditory cue matches the image and is congruent. 
	
```{r echo=TRUE, results='hide', warning=FALSE, message=FALSE}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(readr)
library(lme4)

# Import data
folder_path <- "../data/complete"
csv_files <- list.files(folder_path, pattern = "*.csv", full.names = TRUE)
df_list <- lapply(csv_files, read_csv, show_col_types = FALSE)
original_df <- bind_rows(df_list)

#exclude practice trials 
original_df <- original_df |>
  filter(exp_part == "actual")

# convert the data type and compute correctness of each trial
original_df <- original_df %>%
  mutate(correct_response = as.character(correct_response)) %>%
  mutate(response = as.character(response)) %>%
  mutate(correct = correct_response == response)

# compute the accuracy of each participant
accuracy_table <- original_df %>%
  group_by(ID) %>%
  summarise(accuracy = mean(correct, na.rm = TRUE))
```

First, we check the distribution of accuracy:
```{r echo=TRUE, results='hide', warning=FALSE, message=FALSE}
library(ggplot2)

accuracy_thresh <- 0.9

# Create the plot
ggplot(accuracy_table, aes(x = accuracy)) +
  geom_histogram(
    breaks=seq(0, 1, length.out=21),
    fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = accuracy_thresh, color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Distribution of Accuracy",
    x = "Accuracy",
    y = ""
  ) +
  theme_minimal()
```

`r sum(accuracy_table$accuracy > accuracy_thresh)` people (which are `r round(mean(accuracy_table$accuracy > accuracy_thresh)*100, 0)`% of the total participants) achieved an accuracy above 90%. 

We also check the distribution of response time (within the participants whose accuracy is above 90%):
```{r echo=TRUE, results='hide', warning=FALSE, message=FALSE}
library(ggplot2)

id_pass_qa <- accuracy_table %>%
  filter(accuracy > accuracy_thresh) %>%
  pull(ID)
original_df_with_good_subj <- original_df %>%
  filter(ID %in% id_pass_qa)

# Create the plot
ggplot(original_df_with_good_subj, aes(x = rt)) +
  geom_histogram(
    breaks=seq(0, 2000, length.out=40),
    fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = 250, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = 1500, color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Distribution of Reaction Time",
    x = "Reaction Time",
    y = ""
  ) +
  theme_minimal()

# compute the quantile and other stats
ecdf_func <- ecdf(original_df_with_good_subj$rt)
rt_quantiles <- ecdf_func(c(250, 1500))
rt_pass_mask <- (original_df_with_good_subj$rt > 250) & (original_df_with_good_subj$rt < 1500)

```
`r sum(rt_pass_mask)` trials (which are `r round(mean(rt_pass_mask)*100, 0)`% of the trials) have reaction time within the range of 250-1500 ms.

Now we further preprocess the data, by
- excluding participants who do not achieve an accuracy of 90%

- excluding trials whose reaction time is shorter than 250 ms or longer than 1500 ms

- keeping only trials whose correct answer is 'YES' and people made the correct response.

- recode the trial 'congruency' conditions as 'label', 'congruent', and 'incongruent'


```{r echo=TRUE, results='hide', warning=FALSE, message=FALSE}
combined_df <- original_df
  
## keep only people who passed the QA check
combined_df <- combined_df %>%
  filter(ID %in% id_pass_qa) %>%
  select(-correct) # drop the correctness column

# exclude only correct 'YES' trials 
combined_df <- combined_df %>%
  filter(response == "f") %>%
  filter(correct_response == "f")

# rename "sound_subtype" to "cue"
combined_df <- combined_df %>%
  rename(cue = sound_subtype)

# create congruency column 
combined_df <- combined_df %>%
  mutate(congruency = case_when(
    cue == "label" ~ "label",
    img_subtype %in% c("song", "york", "bongo", "acoustic", "harley", "rotary") & sound_version == "A" ~ "incongruent",
    TRUE ~ "congruent"
  ))

# filter reaction time 
combined_df <- combined_df %>%
  filter(rt >250, rt <1500)

# Prepare data for analysis - keep only columns relevant to the model.
combined_df <- combined_df %>%
  select(rt, ID, sound_category,cue, congruency)

```

### Confirmatory analysis

#### Key Analysis

This analysis is to test whether (1) verbal labels elicit faster responses than sound labels, and (2) congruent sounds elicit faster response than incongruent sounds, with statistical significance, as suggested by the result of Experiment 1A from the original study.

As proposed in our analysis plan, we follow Edminston & Lupyan (2015) fitting reaction time for correct responses on matching trials under three conditions of different cues (verbal label cue, congruent sound cue, incongruent sound cue) using a linear mixed regression model. The model includes random intercepts and random slopes for within-subject factors and random intercepts for repeated items (unique trial types). The main effect (or “contrast of interest”) of the model will be the “condition” variable, which denotes whether a particular trial presented a label, congruent sound, or incongruent sound. 

In the following section, we will report the parameter estimates and confidence intervals for each “contrast of interest”. The parameter estimates for each condition should indicate how much each condition influences reaction time. Furthermore, to quantify our confidence in how much the factors of interest truly accounts for the trends we see, we conduct chi-square tests by comparing whether “nested models–models with and without the factor of interest–on improvement in log-likelihood”. We will the resulting p-value as the statistical significance of the effect.

We expect that the output of our model will be similar to the results presented in Edminston & Lupyan (2015).

The following are the result of fitting a linear mixed-effect regression model and its associated statistics:

```{r echo=TRUE, warning=FALSE, message=FALSE}
library(lmerTest)
library(emmeans)
library(lme4)

# set reference level to congruent
combined_df$congruency <- factor(
  combined_df$congruency, levels = c("congruent", "label", "incongruent"))

# compare between reference (congruent) and the other two values (incongruent and label)
model_full <- lmer(rt ~ congruency + (1 + congruency|ID) + (1|sound_category), data = combined_df)

summary(model_full)
```
[TODO: interpretation]

Get the 95% confidence interval:

```{r echo=TRUE, warning=FALSE, message=FALSE}
#get 95% CI
confint.merMod(model_full,method="Wald")
```
[TODO: interpretation]

The following are (1) the parameter estimates for each auditory cue condition, which implies how much each condition influences reaction time, and (2) our confidence in how much the factors of interest explain the difference in reaction time we observed.

```{r echo=TRUE, warning=FALSE, message=FALSE}
#post-hoc test: examine relationship between incongruent and label
fit_result <- model_full %>% 
  emmeans(
    pairwise ~ congruency, # contrast across 'congruency' conditions
    adjust = "bonferroni" # p-value adjusted
  ) %>% 
  pluck("contrasts")

confint(fit_result, level=0.95)
```
[TODO: interpretation]

To further test whether congruency has a meaningful impact on rt (i.e., whether a reduced model assuming a systematic effect of congruency fits the data better), we apply the chi-square test and the result is as follows:

```{r echo=TRUE, warning=FALSE, message=FALSE}
#chi square test 
model_reduced <- lmer(rt ~ (1 + congruency|ID) + (1|sound_category), data = combined_df)
anova(model_full, model_reduced)
```
[TODO: interpretation]

Compare our result and result in the original study:

```{r}
library(gridExtra)
library(jpeg)
library(grid)

plot_df <- combined_df |>
  mutate(congruency=factor(
    congruency, levels = c("label", "congruent", "incongruent"))) |>
  mutate(congruency = fct_recode(congruency, 
    "Label"="label", 
    "Congruent Sound"="congruent", 
    "Incongruent Sound"="incongruent")
  )

#boxpplot (plot our image)
our_plot <- ggplot(data = plot_df,
       mapping = aes(x = congruency,
                     y = rt,
                     color = congruency, 
                     fill = congruency)) +
  geom_bar(stat = "summary", fun = "mean", 
           width = 1,
           color = "black") + 
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "linerange", 
               color = "black") +
   geom_errorbar(stat = "summary", 
                fun.data = "mean_cl_boot", 
                width = 0.5, 
                size=0.5,
                color = "black") +
  scale_fill_manual(values = c("firebrick", "darkblue", "slategray2")) +
  scale_color_manual(values = c("firebrick", "darkblue", "slategray2")) +
  labs(
    x = "Experiment 1A",
    y = "Verification Speed (ms)",
    color = "Cue Type",
    fill = "Cue Type") +
  coord_cartesian(xlim=c(0, 4), ylim = c(400, 725), expand=FALSE) +
  scale_y_continuous(breaks = seq(400, 700, by = 50), expand=c(0, 25)) +
  theme_classic() +
  theme(
    axis.text.x = element_blank(), 
    axis.text.y = element_text(size = 10, color = "black"),
    axis.title.x = element_text(size = 10, color = "black"), 
    axis.title.y = element_text(size = 10, color = "black"), 
    legend.title = element_text(size = 10, color = "black", face='bold'), 
    legend.text = element_text(size = 10, color = "black"),
    plot.margin = unit(c(0.0, 0.1, 0.0, 0.1), "npc")
  ) +
  theme(
    aspect.ratio = 1.8)

# load the original study
edmiston_img <- rasterGrob(readJPEG("edmiston_exp1a.jpg"), interpolate = TRUE)

# layout side by side
plot_title <- textGrob("Our Result", gp = gpar(fontface = "bold", fontsize = 14))
image_title <- textGrob("Original Study Result", gp = gpar(fontface = "bold", fontsize = 14))

grid.arrange(
  arrangeGrob(image_title, edmiston_img, ncol = 1, heights = c(0.5, 5)),
  arrangeGrob(plot_title, ggplotGrob(our_plot), ncol = 1, heights = c(0.5, 5)),
  ncol = 2, widths=c(1, 1.7))
```

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.

## References
- Lupyan, G., & Thompson-Schill, S. L. (2012). The evocative power of words: Activation of concepts by verbal and nonverbal means. Journal of Experimental Psychology-General, 141(1), 170–186. http://dx.doi.org/10.1037/a0024904.

